{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 256\n",
    "image_directory = r'E:/Project/2024 Project/BrainTumor_Lam/datasets/'\n",
    "\n",
    "no_tumor_images = os.listdir(image_directory + 'sort_crop_no/')\n",
    "yes_tumor_images = os.listdir(image_directory + 'sort_crop_yes/')\n",
    "\n",
    "no_H = os.listdir(image_directory + 'test_data_no/')\n",
    "yes_H = os.listdir(image_directory + 'test_data_yes/')\n",
    "dataset = []\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3242 3242\n"
     ]
    }
   ],
   "source": [
    "for image_name in no_tumor_images:\n",
    "    if image_name.endswith('.jpg'):\n",
    "        image = cv2.imread(image_directory + 'sort_crop_no/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        # image = train_transforms(image)\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        labels.append(0)\n",
    "\n",
    "\n",
    "for image_name in yes_tumor_images:\n",
    "    if image_name.endswith('.jpg'):\n",
    "        image = cv2.imread(image_directory + 'sort_crop_yes/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        # image = test_transforms(image)\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        labels.append(1)\n",
    "\n",
    "for image_name in no_H:\n",
    "    if image_name.endswith('.jpg'):\n",
    "        image = cv2.imread(image_directory + 'test_data_no/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        # image = train_transforms(image)\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        labels.append(0)\n",
    "\n",
    "for image_name in yes_H:\n",
    "    if image_name.endswith('.jpg'):\n",
    "        image = cv2.imread(image_directory + 'test_data_yes/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        # image = test_transforms(image)\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        labels.append(1)\n",
    "\n",
    "print(len(dataset), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "x_train = x_train.permute(0, 3, 1, 2) / 255.0 \n",
    "x_test = x_test.permute(0, 3, 1, 2) / 255.0\n",
    "# chuyển tenser đang từ (batch, height, width, channel) sang (batch, channel, height, width) \n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "# TensorDataset nhận các tensor đầu vào và tạo thành một dataset trong đó, mỗi phần tử là một tuple chứa mẫu dữ liệu và nhãn tương ứng\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainTumorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # 3 channel đầu vào, 32 channel đầu ra, padding = 1 tức là thêm 1 hàng 0 vào 2 bên để giữ nguyên kích thước\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * (INPUT_SIZE // 8) * (INPUT_SIZE // 8), 64) \n",
    "        # INPUT_SIZE // 8 vì sau 3 lớp convolution và 3 lớp pooling, kích thước ảnh giảm đi 8 lần\n",
    "        # 64 là số lượng kênh vào từ lớp tích chập cuối cùng , 64 là số nút đầu ra của lớp fully connected\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        #64 đầu vào và 1 đầu ra\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout rate to 0.3 for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * (INPUT_SIZE // 8) * (INPUT_SIZE // 8))\n",
    "        # view dùng để thay đổi hình dạng của tensor, -1 tức là giữ nguyên số hàng, còn lại là 64 * (INPUT_SIZE // 8) * (INPUT_SIZE // 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BrainTumorNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chức Năng Của optimizer.zero_grad()\n",
    "Đặt Lại Gradient:\n",
    "Trong PyTorch, gradient của các tham số mô hình được tính toán và lưu trữ trong đối tượng Tensor khi bạn gọi loss.backward(). Tuy nhiên, các giá trị gradient này sẽ được cộng dồn (accumulated) qua nhiều lần tính toán gradient liên tiếp nếu bạn không đặt lại chúng.\n",
    "optimizer.zero_grad() được sử dụng để đặt lại tất cả các gradient của các tham số mô hình về 0 trước khi tính toán gradient cho lần huấn luyện tiếp theo.\n",
    "Tránh Tích Lũy Gradient:\n",
    "Nếu bạn không gọi zero_grad(), gradient của các tham số sẽ được cộng dồn qua các lần gọi loss.backward(). Điều này có thể dẫn đến các gradient lớn không mong muốn và làm sai lệch quá trình tối ưu hóa.\n",
    "Đặt lại gradient về 0 đảm bảo rằng gradient của mỗi batch được tính toán độc lập với các batch khác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0769\n",
      "Validation Loss: 0.8120, Val_Accuracy: 89.52%\n",
      "Epoch [2/50], Loss: 0.0621\n",
      "Validation Loss: 0.6758, Val_Accuracy: 91.83%\n",
      "Epoch [3/50], Loss: 0.0624\n",
      "Validation Loss: 0.5098, Val_Accuracy: 91.83%\n",
      "Epoch [4/50], Loss: 0.0647\n",
      "Validation Loss: 0.5011, Val_Accuracy: 91.37%\n",
      "Epoch [5/50], Loss: 0.0511\n",
      "Validation Loss: 0.8356, Val_Accuracy: 92.76%\n",
      "Epoch [6/50], Loss: 0.0477\n",
      "Validation Loss: 0.8661, Val_Accuracy: 91.37%\n",
      "Epoch [7/50], Loss: 0.0344\n",
      "Validation Loss: 0.8298, Val_Accuracy: 91.83%\n",
      "Epoch [8/50], Loss: 0.0528\n",
      "Validation Loss: 0.5553, Val_Accuracy: 91.99%\n",
      "Epoch [9/50], Loss: 0.0368\n",
      "Validation Loss: 0.8260, Val_Accuracy: 92.14%\n",
      "Epoch [10/50], Loss: 0.0250\n",
      "Validation Loss: 0.8449, Val_Accuracy: 92.14%\n",
      "Epoch [11/50], Loss: 0.0253\n",
      "Validation Loss: 0.8643, Val_Accuracy: 91.99%\n",
      "Epoch [12/50], Loss: 0.0265\n",
      "Validation Loss: 0.8868, Val_Accuracy: 92.45%\n",
      "Epoch [13/50], Loss: 0.0178\n",
      "Validation Loss: 0.8870, Val_Accuracy: 92.30%\n",
      "Epoch [14/50], Loss: 0.0213\n",
      "Validation Loss: 0.8879, Val_Accuracy: 92.30%\n",
      "Epoch [15/50], Loss: 0.0327\n",
      "Validation Loss: 0.8927, Val_Accuracy: 92.30%\n",
      "Epoch [16/50], Loss: 0.0212\n",
      "Validation Loss: 0.8907, Val_Accuracy: 92.14%\n",
      "Epoch [17/50], Loss: 0.0162\n",
      "Validation Loss: 0.8906, Val_Accuracy: 92.14%\n",
      "Epoch [18/50], Loss: 0.0208\n",
      "Validation Loss: 0.8907, Val_Accuracy: 92.14%\n",
      "Epoch [19/50], Loss: 0.0211\n",
      "Validation Loss: 0.8907, Val_Accuracy: 92.14%\n",
      "Epoch [20/50], Loss: 0.0203\n",
      "Validation Loss: 0.8906, Val_Accuracy: 92.14%\n",
      "Epoch [21/50], Loss: 0.0214\n",
      "Validation Loss: 0.8908, Val_Accuracy: 92.14%\n",
      "Epoch [22/50], Loss: 0.0154\n",
      "Validation Loss: 0.8912, Val_Accuracy: 92.14%\n",
      "Epoch [23/50], Loss: 0.0187\n",
      "Validation Loss: 0.8915, Val_Accuracy: 92.14%\n",
      "Epoch [24/50], Loss: 0.0194\n",
      "Validation Loss: 0.8917, Val_Accuracy: 92.14%\n",
      "Epoch [25/50], Loss: 0.0153\n",
      "Validation Loss: 0.8917, Val_Accuracy: 92.30%\n",
      "Epoch [26/50], Loss: 0.0223\n",
      "Validation Loss: 0.8919, Val_Accuracy: 92.30%\n",
      "Epoch [27/50], Loss: 0.0144\n",
      "Validation Loss: 0.8923, Val_Accuracy: 92.14%\n",
      "Epoch [28/50], Loss: 0.0241\n",
      "Validation Loss: 0.8918, Val_Accuracy: 92.14%\n",
      "Epoch [29/50], Loss: 0.0179\n",
      "Validation Loss: 0.8924, Val_Accuracy: 92.14%\n",
      "Epoch [30/50], Loss: 0.0161\n",
      "Validation Loss: 0.8931, Val_Accuracy: 92.14%\n",
      "Epoch [31/50], Loss: 0.0213\n",
      "Validation Loss: 0.8932, Val_Accuracy: 92.14%\n",
      "Epoch [32/50], Loss: 0.0178\n",
      "Validation Loss: 0.8935, Val_Accuracy: 92.14%\n",
      "Epoch [33/50], Loss: 0.0171\n",
      "Validation Loss: 0.8936, Val_Accuracy: 92.14%\n",
      "Epoch [34/50], Loss: 0.0174\n",
      "Validation Loss: 0.8941, Val_Accuracy: 92.14%\n",
      "Epoch [35/50], Loss: 0.0217\n",
      "Validation Loss: 0.8939, Val_Accuracy: 92.14%\n",
      "Epoch [36/50], Loss: 0.0203\n",
      "Validation Loss: 0.8943, Val_Accuracy: 92.14%\n",
      "Epoch [37/50], Loss: 0.0202\n",
      "Validation Loss: 0.8944, Val_Accuracy: 92.14%\n",
      "Epoch [38/50], Loss: 0.0254\n",
      "Validation Loss: 0.8941, Val_Accuracy: 92.14%\n",
      "Epoch [39/50], Loss: 0.0205\n",
      "Validation Loss: 0.8939, Val_Accuracy: 92.14%\n",
      "Epoch [40/50], Loss: 0.0234\n",
      "Validation Loss: 0.8943, Val_Accuracy: 92.14%\n",
      "Epoch [41/50], Loss: 0.0196\n",
      "Validation Loss: 0.8947, Val_Accuracy: 92.14%\n",
      "Epoch [42/50], Loss: 0.0214\n",
      "Validation Loss: 0.8947, Val_Accuracy: 92.14%\n",
      "Epoch [43/50], Loss: 0.0189\n",
      "Validation Loss: 0.8949, Val_Accuracy: 92.14%\n",
      "Epoch [44/50], Loss: 0.0192\n",
      "Validation Loss: 0.8952, Val_Accuracy: 92.14%\n",
      "Epoch [45/50], Loss: 0.0176\n",
      "Validation Loss: 0.8955, Val_Accuracy: 92.14%\n",
      "Epoch [46/50], Loss: 0.0179\n",
      "Validation Loss: 0.8970, Val_Accuracy: 92.14%\n",
      "Epoch [47/50], Loss: 0.0132\n",
      "Validation Loss: 0.8972, Val_Accuracy: 92.14%\n",
      "Epoch [48/50], Loss: 0.0162\n",
      "Validation Loss: 0.8976, Val_Accuracy: 92.14%\n",
      "Epoch [49/50], Loss: 0.0184\n",
      "Validation Loss: 0.8980, Val_Accuracy: 92.14%\n",
      "Epoch [50/50], Loss: 0.0191\n",
      "Validation Loss: 0.8982, Val_Accuracy: 92.14%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Assuming model, train_loader, test_loader, device are defined\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        outputs = model(inputs).squeeze()  # Forward pass\n",
    "        \n",
    "        # Ensure outputs and labels have the same shape\n",
    "        if outputs.dim() != labels.dim():\n",
    "            outputs = outputs.view(-1)  # Flatten if needed\n",
    "            labels = labels.view(-1)  # Flatten if needed\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            \n",
    "            # Ensure outputs and labels have the same shape\n",
    "            if outputs.dim() != labels.dim():\n",
    "                outputs = outputs.view(-1)  # Flatten if needed\n",
    "                labels = labels.view(-1)  # Flatten if needed\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(test_loader):.4f}, Val_Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    # Adjust learning rate after validation phase\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "# Save the model\n",
    "save_path = 'E:/Project/2024 Project/BrainTumor_Lam/models/brain_tumor_pytorch_v1.pth'\n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9125874125874126\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs > 0.5).cpu().numpy().astype(int)\n",
    "        y_pred.extend(predicted)\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
